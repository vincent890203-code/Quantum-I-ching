"""Quantum I-Ching å°ˆæ¡ˆåŸºæº–æ¯”è¼ƒå¯¦é©—æ¨¡çµ„.

æ­¤æ¨¡çµ„ç”¨æ–¼é©—è­‰æ˜“ç¶“ç‰¹å¾µï¼ˆå¦è±¡ Embeddingï¼‰æ˜¯å¦çœŸçš„å…·æœ‰é æ¸¬èƒ½åŠ›ï¼Œ
é‚„æ˜¯æ¨¡å‹å¯¦ä½œå­˜åœ¨å•é¡Œã€‚

å¯¦é©—è¨­è¨ˆï¼š
1. Baseline Model (PureLSTM): åƒ…ä½¿ç”¨æ•¸å€¼ç‰¹å¾µ
2. Quantum Model (QuantumLSTM): ä½¿ç”¨æ•¸å€¼ç‰¹å¾µ + é›™æµå¦è±¡ Embedding
3. åœ¨ç›¸åŒè³‡æ–™ã€ç›¸åŒè¶…åƒæ•¸ä¸‹ä¸¦è¡Œè¨“ç·´ï¼Œæ¯”è¼ƒé©—è­‰æå¤±å’Œæº–ç¢ºç‡
"""

import os
import random
from typing import Dict, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, TensorDataset
from tqdm.auto import tqdm

from config import settings
from data_loader import MarketDataLoader
from data_processor import DataProcessor
from market_encoder import MarketEncoder
from model_lstm import QuantumLSTM, QuantumTrainer


class PureLSTM(nn.Module):
    """ç´”æ•¸å€¼ç‰¹å¾µ LSTM æ¨¡å‹ï¼ˆBaselineï¼‰.

    åƒ…ä½¿ç”¨æ•¸å€¼æŠ€è¡“æŒ‡æ¨™ï¼Œä¸ä½¿ç”¨æ˜“ç¶“ç‰¹å¾µã€‚
    ä»£è¡¨å‚³çµ±æŠ€è¡“åˆ†ææ–¹æ³•ã€‚
    """

    def __init__(
        self,
        num_features: int = 4,  # åƒ…æ•¸å€¼ç‰¹å¾µï¼šClose, Volume, RVOL, Daily_Return
        hidden_dim: int = 32,  # é™ä½åˆ° 32 ä»¥é˜²æ­¢éæ“¬åˆ
        num_layers: int = 2,
        dropout: float = 0.2,
    ) -> None:
        """åˆå§‹åŒ– PureLSTM æ¨¡å‹.

        Args:
            num_features: æ•¸å€¼ç‰¹å¾µæ•¸é‡ï¼ˆé è¨­ 4: Close, Volume, RVOL, Daily_Returnï¼‰ã€‚
            hidden_dim: LSTM éš±è—å±¤ç¶­åº¦ï¼ˆé è¨­ 32ï¼‰ã€‚
            num_layers: LSTM å †ç–Šå±¤æ•¸ã€‚
            dropout: dropout æ¯”ä¾‹ã€‚
        """
        super().__init__()

        # åƒ…ä½¿ç”¨æ•¸å€¼ç‰¹å¾µï¼Œç„¡å¦è±¡ Embedding
        input_dim: int = num_features

        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0,
        )

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘å‚³æ’­ï¼ˆåƒ…æ•¸å€¼ç‰¹å¾µï¼‰.

        Args:
            x: æ•¸å€¼ç‰¹å¾µï¼Œå½¢ç‹€ç‚º (batch_size, seq_len, num_features)ã€‚

        Returns:
            é æ¸¬æ©Ÿç‡ï¼Œå½¢ç‹€ç‚º (batch_size, 1)ã€‚
        """
        # LSTM è¼¸å‡º: output å½¢ç‹€ (batch_size, seq_len, hidden_dim)
        output, _ = self.lstm(x)

        # å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡º
        last_output: torch.Tensor = output[:, -1, :]

        # Dropout + å…¨é€£æ¥ + Sigmoid
        out: torch.Tensor = self.dropout(last_output)
        out = self.fc(out)
        out = self.sigmoid(out)

        return out


def set_random_seed(seed: int = 42) -> None:
    """è¨­ç½®éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿å¯¦é©—å¯é‡ç¾.

    Args:
        seed: éš¨æ©Ÿç¨®å­å€¼ã€‚
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    epochs: int = 20,
    learning_rate: float = 0.0005,
    weight_decay: float = 2e-5,
    patience: int = 15,
    min_delta: float = 0.0002,
    model_name: str = "Model",
) -> Dict[str, float]:
    """è¨“ç·´æ¨¡å‹ä¸¦è¿”å›æœ€çµ‚é©—è­‰æŒ‡æ¨™.

    Args:
        model: è¦è¨“ç·´çš„æ¨¡å‹ã€‚
        train_loader: è¨“ç·´è³‡æ–™ DataLoaderã€‚
        val_loader: é©—è­‰è³‡æ–™ DataLoaderã€‚
        epochs: è¨“ç·´è¼ªæ•¸ã€‚
        learning_rate: å­¸ç¿’ç‡ã€‚
        weight_decay: L2 æ­£å‰‡åŒ–ä¿‚æ•¸ã€‚
        patience: early stopping çš„å®¹å¿ epoch æ•¸ã€‚
        min_delta: early stopping åˆ¤æ–·æ”¹å–„çš„æœ€å°å·®å€¼ã€‚
        model_name: æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼æ—¥èªŒï¼‰ã€‚

    Returns:
        åŒ…å«æœ€çµ‚é©—è­‰æå¤±å’Œæº–ç¢ºç‡çš„å­—å…¸ã€‚
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.7, patience=4, min_lr=1e-6, verbose=False
    )

    best_val_loss = float("inf")
    epochs_without_improve = 0

    for epoch in range(1, epochs + 1):
        # è¨“ç·´éšæ®µ
        model.train()
        train_loss_sum = 0.0
        train_batches = 0

        for batch in train_loader:
            x, y = batch
            x = x.to(device)
            y = y.to(device)
            
            outputs = model(x)
            y = y.view_as(outputs)
            loss = criterion(outputs, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss_sum += loss.item()
            train_batches += 1

        avg_train_loss = train_loss_sum / max(train_batches, 1)

        # é©—è­‰éšæ®µ
        model.eval()
        val_loss_sum = 0.0
        val_batches = 0
        correct = 0
        total = 0
        # ç”¨æ–¼è¨ˆç®— Precision å’Œ Recall
        true_positive = 0
        false_positive = 0
        false_negative = 0

        with torch.no_grad():
            for batch in val_loader:
                x, y = batch
                x = x.to(device)
                y = y.to(device)
                
                outputs = model(x)
                y = y.view_as(outputs)
                loss = criterion(outputs, y)

                val_loss_sum += loss.item()
                val_batches += 1

                predicted = (outputs >= 0.5).float()
                correct += (predicted == y).sum().item()
                total += y.numel()
                
                # è¨ˆç®— Precision å’Œ Recallï¼ˆé‡å°æ­£é¡ï¼šé«˜æ³¢å‹•ï¼‰
                # TP: é æ¸¬ç‚ºé«˜æ³¢å‹•ä¸”å¯¦éš›ç‚ºé«˜æ³¢å‹•
                # FP: é æ¸¬ç‚ºé«˜æ³¢å‹•ä½†å¯¦éš›ç‚ºä½æ³¢å‹•
                # FN: é æ¸¬ç‚ºä½æ³¢å‹•ä½†å¯¦éš›ç‚ºé«˜æ³¢å‹•
                true_positive += ((predicted == 1) & (y == 1)).sum().item()
                false_positive += ((predicted == 1) & (y == 0)).sum().item()
                false_negative += ((predicted == 0) & (y == 1)).sum().item()

        avg_val_loss = val_loss_sum / max(val_batches, 1)
        val_accuracy = correct / max(total, 1)
        
        # è¨ˆç®— Precision å’Œ Recall
        precision = true_positive / max(true_positive + false_positive, 1)
        recall = true_positive / max(true_positive + false_negative, 1)
        f1_score = 2 * (precision * recall) / max(precision + recall, 1e-8)

        scheduler.step(avg_val_loss)

        # Early stopping åˆ¤æ–·
        if avg_val_loss < best_val_loss - min_delta:
            best_val_loss = avg_val_loss
            epochs_without_improve = 0
        else:
            epochs_without_improve += 1

        if epoch % 5 == 0 or epoch == 1:
            print(
                f"  [{model_name}] Epoch [{epoch}/{epochs}] - "
                f"Train Loss: {avg_train_loss:.4f} - "
                f"Val Loss: {avg_val_loss:.4f} - "
                f"Val Acc: {val_accuracy:.4f} - "
                f"Precision: {precision:.4f} - "
                f"Recall: {recall:.4f}"
            )

        if epochs_without_improve >= patience:
            print(f"  [{model_name}] Early stopping at epoch {epoch}")
            break

    return {
        "val_loss": best_val_loss,
        "val_accuracy": val_accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score,
        "final_train_loss": avg_train_loss,
        "final_val_loss": avg_val_loss,
    }


def analyze_confidence_tiers(
    model: nn.Module,
    val_loader: DataLoader,
    model_name: str = "Model",
    thresholds: list = [0.5, 0.55, 0.6, 0.65, 0.7]
) -> Dict[str, Dict[float, float]]:
    """åˆ†æä¸åŒä¿¡å¿ƒé–¾å€¼ä¸‹çš„æ¨¡å‹è¡¨ç¾.
    
    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹ã€‚
        val_loader: é©—è­‰è³‡æ–™ DataLoaderã€‚
        model_name: æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼æ—¥èªŒï¼‰ã€‚
        thresholds: è¦åˆ†æçš„ä¿¡å¿ƒé–¾å€¼åˆ—è¡¨ã€‚
    
    Returns:
        åŒ…å«æ¯å€‹é–¾å€¼ä¸‹æŒ‡æ¨™çš„å­—å…¸ï¼š
        {
            "num_trades": {threshold: count},
            "win_rate": {threshold: rate},
            "precision": {threshold: precision}
        }
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()
    
    # æ”¶é›†æ‰€æœ‰é æ¸¬æ©Ÿç‡å’ŒçœŸå¯¦æ¨™ç±¤
    all_probs = []
    all_labels = []
    
    with torch.no_grad():
        for batch in val_loader:
            x, y = batch
            x = x.to(device)
            y = y.to(device)
            
            outputs = model(x)
            y = y.view_as(outputs)
            
            # æ”¶é›†é æ¸¬æ©Ÿç‡å’Œæ¨™ç±¤
            all_probs.append(outputs.cpu().numpy().flatten())
            all_labels.append(y.cpu().numpy().flatten())
    
    # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)
    
    # è¨ˆç®—æ¯å€‹é–¾å€¼ä¸‹çš„æŒ‡æ¨™
    results = {
        "num_trades": {},
        "win_rate": {},
        "precision": {}
    }
    
    for threshold in thresholds:
        # ç¯©é¸é«˜ä¿¡å¿ƒé æ¸¬ï¼ˆé æ¸¬æ©Ÿç‡ >= é–¾å€¼ï¼‰
        high_confidence_mask = all_probs >= threshold
        num_trades = high_confidence_mask.sum()
        
        if num_trades == 0:
            # å¦‚æœæ²’æœ‰é«˜ä¿¡å¿ƒé æ¸¬ï¼Œè¨­ç‚º NaN
            results["num_trades"][threshold] = 0
            results["win_rate"][threshold] = float('nan')
            results["precision"][threshold] = float('nan')
        else:
            # ç²å–é«˜ä¿¡å¿ƒé æ¸¬çš„æ¨™ç±¤
            high_conf_labels = all_labels[high_confidence_mask]
            
            # è¨ˆç®— Win Rateï¼ˆé€™äº›ä¿¡è™Ÿçš„æº–ç¢ºç‡ï¼‰
            # å°æ–¼é«˜ä¿¡å¿ƒé æ¸¬ï¼Œæˆ‘å€‘é æ¸¬ç‚ºé«˜æ³¢å‹•ï¼ˆlabel=1ï¼‰
            # Win Rate = å¯¦éš›ç‚ºé«˜æ³¢å‹•çš„æ¯”ä¾‹
            win_rate = high_conf_labels.mean()  # æ¨™ç±¤ç‚º 1 çš„æ¯”ä¾‹
            
            # è¨ˆç®— Precisionï¼ˆé æ¸¬ç‚ºé«˜æ³¢å‹•æ™‚ï¼Œå¯¦éš›ç‚ºé«˜æ³¢å‹•çš„æ¯”ä¾‹ï¼‰
            # å°æ–¼é«˜ä¿¡å¿ƒé æ¸¬ï¼ˆprob >= thresholdï¼‰ï¼Œæˆ‘å€‘é æ¸¬ç‚ºé«˜æ³¢å‹•
            # Precision = å¯¦éš›ç‚ºé«˜æ³¢å‹•ï¼ˆlabel == 1ï¼‰çš„æ¯”ä¾‹
            precision = high_conf_labels.mean()  # èˆ‡ Win Rate ç›¸åŒï¼Œå› ç‚ºæˆ‘å€‘é æ¸¬ç‚ºé«˜æ³¢å‹•
            
            results["num_trades"][threshold] = num_trades
            results["win_rate"][threshold] = win_rate
            results["precision"][threshold] = precision
    
    return results


def run_sanity_check() -> bool:
    """åŸ·è¡Œå¥å…¨æ€§æª¢æŸ¥ï¼šé©—è­‰æ¨¡å‹èƒ½å¦åœ¨å°è³‡æ–™é›†ä¸Šéæ“¬åˆ.

    Returns:
        True å¦‚æœé€šéæª¢æŸ¥ï¼ŒFalse å¦‚æœå¤±æ•—ã€‚
    """
    print("=" * 60)
    print("å¥å…¨æ€§æª¢æŸ¥ (Sanity Check)")
    print("=" * 60)
    print("\nç›®æ¨™ï¼šé©—è­‰æ¨¡å‹èƒ½å¦åœ¨å°è³‡æ–™é›†ï¼ˆ50 ç­†ï¼‰ä¸Šéæ“¬åˆ")
    print("é æœŸï¼šè¨“ç·´æå¤±æ‡‰é™è‡³æ¥è¿‘ 0ï¼ˆ< 0.01ï¼‰")
    print(f"è¨­å®šï¼š200 epochsï¼Œé æ¸¬ T+{settings.PREDICTION_WINDOW} æ³¢å‹•æ€§çªç ´")
    print(f"ä½¿ç”¨æœ€ä½³åƒæ•¸ï¼šsequence_length={settings.SEQUENCE_LENGTH}\n")

    set_random_seed(42)

    # è¼‰å…¥è³‡æ–™
    loader = MarketDataLoader()
    default_symbol = (
        settings.TARGET_TICKERS[0] if settings.TARGET_TICKERS else "NVDA"
    )
    raw_data = loader.fetch_data(tickers=[default_symbol])

    if raw_data.empty:
        print(f"[ERROR] ç„¡æ³•ç²å– {default_symbol} çš„å¸‚å ´è³‡æ–™")
        return False

    encoder = MarketEncoder()
    encoded_data = encoder.generate_hexagrams(raw_data)

    if encoded_data.empty:
        print("[ERROR] ç·¨ç¢¼å¾Œçš„è³‡æ–™ç‚ºç©º")
        return False

    # æº–å‚™è³‡æ–™ï¼ˆä½¿ç”¨æœ€ä½³åƒæ•¸ï¼šsequence_length=30ï¼‰
    processor = DataProcessor(sequence_length=settings.SEQUENCE_LENGTH, prediction_window=settings.PREDICTION_WINDOW)
    try:
        X, y = processor.prepare_data(encoded_data)
    except ValueError as e:
        print(f"[ERROR] è³‡æ–™æº–å‚™å¤±æ•—: {e}")
        return False

    # å–å‰ 50 ç­†ä½œç‚ºå°è³‡æ–™é›†
    tiny_size = min(50, len(X))
    X_tiny = X[:tiny_size]
    y_tiny = y[:tiny_size]

    # è½‰æ›ç‚ºå¼µé‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X_tensor = torch.tensor(X_tiny, dtype=torch.float32)
    y_tensor = torch.tensor(y_tiny, dtype=torch.float32)

    # æª¢æŸ¥æ˜¯å¦æœ‰ NaN
    if torch.isnan(X_tensor).any() or torch.isnan(y_tensor).any():
        print("[ERROR] ç™¼ç¾ NaN å€¼åœ¨å¼µé‡ä¸­ï¼")
        return False

    # å‰µå»º DataLoaderï¼ˆä½¿ç”¨ç›¸åŒè³‡æ–™ä½œç‚ºè¨“ç·´å’Œé©—è­‰ï¼‰
    dataset = TensorDataset(X_tensor, y_tensor)
    tiny_loader = DataLoader(dataset, batch_size=16, shuffle=False)

    # æª¢æŸ¥æ¨™ç±¤åˆ†å¸ƒ
    label_dist = np.bincount(y_tiny.flatten().astype(int))
    print(f"[INFO] æ¨™ç±¤åˆ†å¸ƒ: é«˜æ³¢å‹•={label_dist[1] if len(label_dist) > 1 else 0}, ä½æ³¢å‹•={label_dist[0]}")
    if len(label_dist) > 1 and (label_dist[0] == 0 or label_dist[1] == 0):
        print("[WARNING] æ¨™ç±¤å®Œå…¨ä¸å¹³è¡¡ï¼é€™æœƒå°è‡´æ¨¡å‹ç„¡æ³•å­¸ç¿’ã€‚")
        return False
    
    # è¨“ç·´æ¨¡å‹ - ä½¿ç”¨æœ€ä½³è¶…åƒæ•¸
    model = QuantumLSTM(
        num_features=9, 
        hidden_dim=settings.HIDDEN_DIM,  # 256
        num_layers=settings.NUM_LAYERS,  # 1
        dropout=settings.DROPOUT  # 0.35
    ).to(device)
    
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=settings.LEARNING_RATE)  # 0.001
    
    # æ·»åŠ å­¸ç¿’ç‡èª¿åº¦å™¨ï¼ˆå¯é¸ï¼Œä½†å…ˆä¸ç”¨ï¼‰
    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)

    model.train()
    best_loss = float('inf')
    patience_counter = 0
    
    # æ›´æ–°ï¼šå¢åŠ åˆ° 200 epochs ä»¥ç¢ºä¿æ”¶æ–‚
    for epoch in range(1, 201):
        epoch_loss = 0.0
        batch_count = 0

        for batch in tiny_loader:
            x, y_batch = batch
            x = x.to(device)
            y_batch = y_batch.to(device)

            # å†æ¬¡æª¢æŸ¥ NaN
            if torch.isnan(x).any() or torch.isnan(y_batch).any():
                print(f"[ERROR] Epoch {epoch}: ç™¼ç¾ NaN å€¼åœ¨ batch ä¸­ï¼")
                return False

            outputs = model(x)
            y_batch = y_batch.view_as(outputs)
            loss = criterion(outputs, y_batch)

            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªä»¥é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()

            epoch_loss += loss.item()
            batch_count += 1

        avg_loss = epoch_loss / max(batch_count, 1)
        
        # è¿½è¹¤æœ€ä½³æå¤±
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1

        if epoch % 20 == 0 or epoch == 1:
            print(f"  Epoch {epoch}: Train Loss = {avg_loss:.6f}, Best = {best_loss:.6f}")
        
        # å¦‚æœæå¤±å·²ç¶“å¾ˆä½ï¼Œæå‰åœæ­¢
        if avg_loss < 0.001:
            print(f"  [SUCCESS] æå¤±å·²é™è‡³ {avg_loss:.6f} < 0.001ï¼Œæå‰åœæ­¢")
            break
        
        # å¦‚æœ50å€‹epochæ²’æœ‰æ”¹å–„ï¼Œæé«˜å­¸ç¿’ç‡
        if patience_counter >= 50 and epoch < 150:
            current_lr = optimizer.param_groups[0]['lr']
            new_lr = current_lr * 2
            optimizer.param_groups[0]['lr'] = new_lr
            print(f"  [INFO] æé«˜å­¸ç¿’ç‡è‡³ {new_lr:.6f}")
            patience_counter = 0

    final_loss = avg_loss

    print(f"\næœ€çµ‚è¨“ç·´æå¤±: {final_loss:.6f}")
    if final_loss < 0.01:
        print("[PASS] å¥å…¨æ€§æª¢æŸ¥é€šéï¼šæ¨¡å‹èƒ½å¤ åœ¨å°è³‡æ–™é›†ä¸Šéæ“¬åˆ")
        print("       é€™è¡¨ç¤ºæ¨¡å‹å¯¦ä½œæ­£ç¢ºï¼Œèƒ½å¤ å­¸ç¿’è³‡æ–™æ¨¡å¼ã€‚\n")
        return True
    else:
        print("[FAIL] å¥å…¨æ€§æª¢æŸ¥å¤±æ•—ï¼šæ¨¡å‹ç„¡æ³•åœ¨å°è³‡æ–™é›†ä¸Šéæ“¬åˆ")
        print("       å¯èƒ½åŸå› ï¼šç¨‹å¼ç¢¼éŒ¯èª¤ã€è³‡æ–™å•é¡Œã€æˆ–æ¨¡å‹æ¶æ§‹å•é¡Œã€‚\n")
        return False


def run_comparison() -> Dict[str, Dict[str, float]]:
    """åŸ·è¡ŒåŸºæº–æ¯”è¼ƒå¯¦é©—.

    Returns:
        åŒ…å«å…©å€‹æ¨¡å‹çµæœçš„å­—å…¸ã€‚
    """
    print("=" * 60)
    print("åŸºæº–æ¯”è¼ƒå¯¦é©— (Baseline Comparison)")
    print("=" * 60)
    print("\nç›®æ¨™ï¼šæ¯”è¼ƒ QuantumLSTMï¼ˆæ˜“ç¶“ç‰¹å¾µï¼‰vs PureLSTMï¼ˆåƒ…æ•¸å€¼ç‰¹å¾µï¼‰")
    print("æ–¹æ³•ï¼šç‰¹å¾µå·¥ç¨‹ï¼ˆæ‰‹å·¥ç‰¹å¾µï¼‰æ›¿ä»£ Embedding")
    print("é æ¸¬ç›®æ¨™ï¼šæ³¢å‹•æ€§çªç ´ï¼ˆVolatility Breakoutï¼‰")
    print("æ¨™ç±¤å®šç¾©ï¼šé«˜æ³¢å‹• = |5å¤©å ±é…¬ç‡| > 3%, ä½æ³¢å‹• = |5å¤©å ±é…¬ç‡| <= 3%")
    print("é æ¸¬æ™‚é–“ç¯„åœï¼šT+5 (5å¤©å¾Œ)")
    print("æ¢ä»¶ï¼šç›¸åŒè³‡æ–™ã€ç›¸åŒè¶…åƒæ•¸ã€ç›¸åŒéš¨æ©Ÿç¨®å­\n")

    set_random_seed(42)

    # è¼‰å…¥è³‡æ–™
    loader = MarketDataLoader()
    default_symbol = (
        settings.TARGET_TICKERS[0] if settings.TARGET_TICKERS else "NVDA"
    )
    raw_data = loader.fetch_data(tickers=[default_symbol])

    if raw_data.empty:
        raise ValueError(f"ç„¡æ³•ç²å– {default_symbol} çš„å¸‚å ´è³‡æ–™")

    encoder = MarketEncoder()
    encoded_data = encoder.generate_hexagrams(raw_data)

    if encoded_data.empty:
        raise ValueError("ç·¨ç¢¼å¾Œçš„è³‡æ–™ç‚ºç©º")

    # æº–å‚™è³‡æ–™ï¼ˆä½¿ç”¨æœ€ä½³åƒæ•¸ï¼šsequence_length=30ï¼‰
    processor = DataProcessor(sequence_length=settings.SEQUENCE_LENGTH, prediction_window=settings.PREDICTION_WINDOW)
    X, y = processor.prepare_data(encoded_data)

    # åˆ†å‰²è³‡æ–™ï¼ˆä½¿ç”¨ç›¸åŒçš„åˆ†å‰²é‚è¼¯ï¼‰
    split_idx = int(len(X) * 0.8)

    X_train = X[:split_idx]
    y_train = y[:split_idx]

    X_val = X[split_idx:]
    y_val = y[split_idx:]

    # è½‰æ›ç‚ºå¼µé‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # QuantumLSTM DataLoaderï¼ˆä½¿ç”¨æ‰€æœ‰ 9 å€‹ç‰¹å¾µï¼‰
    X_train_t = torch.tensor(X_train, dtype=torch.float32)
    y_train_t = torch.tensor(y_train, dtype=torch.float32)

    X_val_t = torch.tensor(X_val, dtype=torch.float32)
    y_val_t = torch.tensor(y_val, dtype=torch.float32)

    quantum_train_dataset = TensorDataset(X_train_t, y_train_t)
    quantum_val_dataset = TensorDataset(X_val_t, y_val_t)

    quantum_train_loader = DataLoader(
        quantum_train_dataset, batch_size=32, shuffle=True
    )
    quantum_val_loader = DataLoader(
        quantum_val_dataset, batch_size=32, shuffle=False
    )

    # PureLSTM DataLoaderï¼ˆåƒ…ä½¿ç”¨å‰ 4 å€‹æ•¸å€¼ç‰¹å¾µï¼‰
    # å‰µå»ºåªåŒ…å«æ•¸å€¼ç‰¹å¾µçš„è³‡æ–™é›†
    X_train_numerical = X_train[:, :, :4]  # åªå–å‰ 4 å€‹ç‰¹å¾µ
    X_val_numerical = X_val[:, :, :4]
    
    X_train_numerical_t = torch.tensor(X_train_numerical, dtype=torch.float32)
    X_val_numerical_t = torch.tensor(X_val_numerical, dtype=torch.float32)

    pure_train_dataset = TensorDataset(X_train_numerical_t, y_train_t)
    pure_val_dataset = TensorDataset(X_val_numerical_t, y_val_t)

    pure_train_loader = DataLoader(
        pure_train_dataset, batch_size=32, shuffle=True
    )
    pure_val_loader = DataLoader(
        pure_val_dataset, batch_size=32, shuffle=False
    )

    # è¶…åƒæ•¸ï¼ˆä½¿ç”¨æœ€ä½³åƒæ•¸ï¼‰
    hyperparams = {
        "epochs": 20,
        "learning_rate": settings.LEARNING_RATE,  # 0.001
        "weight_decay": 1e-5,
        "patience": 10,
        "min_delta": 0.0001,
    }

    print(f"è¶…åƒæ•¸è¨­å®šï¼ˆä½¿ç”¨ Optuna æœ€ä½³åƒæ•¸ï¼‰ï¼š")
    print(f"  Sequence Length: {settings.SEQUENCE_LENGTH} (æœˆé€±æœŸ)")
    print(f"  Hidden Dim: {settings.HIDDEN_DIM}")
    print(f"  Num Layers: {settings.NUM_LAYERS}")
    print(f"  Dropout: {settings.DROPOUT}")
    print(f"  Epochs: {hyperparams['epochs']}")
    print(f"  Learning Rate: {hyperparams['learning_rate']}")
    print(f"  Weight Decay: {hyperparams['weight_decay']}")
    print(f"  Patience: {hyperparams['patience']}")
    print(f"  Min Delta: {hyperparams['min_delta']}\n")

    # è¨“ç·´ QuantumLSTMï¼ˆä½¿ç”¨æœ€ä½³è¶…åƒæ•¸ï¼‰
    print("=" * 60)
    print("è¨“ç·´ QuantumLSTMï¼ˆç‰¹å¾µå·¥ç¨‹æ–¹æ³•ï¼šæ•¸å€¼ç‰¹å¾µ + æ˜“ç¶“æ‰‹å·¥ç‰¹å¾µï¼‰")
    print("é æ¸¬ç›®æ¨™ï¼šæ³¢å‹•æ€§çªç ´ï¼ˆé«˜æ³¢å‹• vs ä½æ³¢å‹•ï¼‰")
    print(f"ä½¿ç”¨æœ€ä½³è¶…åƒæ•¸ï¼šseq_len={settings.SEQUENCE_LENGTH}, hidden_dim={settings.HIDDEN_DIM}, "
          f"layers={settings.NUM_LAYERS}, dropout={settings.DROPOUT}")
    print("=" * 60)
    quantum_model = QuantumLSTM(
        num_features=9, 
        hidden_dim=settings.HIDDEN_DIM,  # 256
        num_layers=settings.NUM_LAYERS,  # 1
        dropout=settings.DROPOUT  # 0.35
    )
    quantum_results = train_model(
        quantum_model,
        quantum_train_loader,
        quantum_val_loader,
        model_name="Quantum",
        **hyperparams,
    )
    
    # åˆ†æ QuantumLSTM çš„ä¿¡å¿ƒé–¾å€¼è¡¨ç¾
    print("\n" + "=" * 60)
    print("QuantumLSTM ä¿¡å¿ƒé–¾å€¼åˆ†æ (Confidence Tier Analysis)")
    print("=" * 60)
    quantum_confidence = analyze_confidence_tiers(
        quantum_model,
        quantum_val_loader,
        model_name="Quantum",
        thresholds=[0.5, 0.55, 0.6, 0.65, 0.7]
    )
    
    print(f"\n{'é–¾å€¼':<10} {'# äº¤æ˜“æ¬¡æ•¸':<15} {'Win Rate':<15} {'Precision':<15}")
    print("-" * 60)
    for threshold in [0.5, 0.55, 0.6, 0.65, 0.7]:
        num_trades = quantum_confidence["num_trades"][threshold]
        win_rate = quantum_confidence["win_rate"][threshold]
        precision = quantum_confidence["precision"][threshold]
        
        win_rate_str = f"{win_rate:.4f}" if not np.isnan(win_rate) else "N/A"
        precision_str = f"{precision:.4f}" if not np.isnan(precision) else "N/A"
        
        print(f"{threshold:<10.2f} {num_trades:<15} {win_rate_str:<15} {precision_str:<15}")

    # é‡ç½®éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿å…¬å¹³æ¯”è¼ƒ
    set_random_seed(42)

    # è¨“ç·´ PureLSTMï¼ˆBaselineï¼Œä½¿ç”¨ç›¸åŒçš„ sequence_length ä»¥ç¢ºä¿å…¬å¹³æ¯”è¼ƒï¼‰
    print("\n" + "=" * 60)
    print("è¨“ç·´ PureLSTMï¼ˆåƒ…æ•¸å€¼ç‰¹å¾µï¼šBaselineï¼‰")
    print(f"ä½¿ç”¨ç›¸åŒ sequence_length={settings.SEQUENCE_LENGTH} ä»¥ç¢ºä¿å…¬å¹³æ¯”è¼ƒ")
    print("=" * 60)
    pure_model = PureLSTM(
        num_features=4, 
        hidden_dim=settings.HIDDEN_DIM,  # 256ï¼ˆèˆ‡ QuantumLSTM ç›¸åŒï¼‰
        num_layers=settings.NUM_LAYERS,  # 1
        dropout=settings.DROPOUT  # 0.35
    )
    pure_results = train_model(
        pure_model,
        pure_train_loader,
        pure_val_loader,
        model_name="Baseline",
        **hyperparams,
    )
    
    # åˆ†æ PureLSTM çš„ä¿¡å¿ƒé–¾å€¼è¡¨ç¾
    print("\n" + "=" * 60)
    print("PureLSTM ä¿¡å¿ƒé–¾å€¼åˆ†æ (Confidence Tier Analysis)")
    print("=" * 60)
    baseline_confidence = analyze_confidence_tiers(
        pure_model,
        pure_val_loader,
        model_name="Baseline",
        thresholds=[0.5, 0.55, 0.6, 0.65, 0.7]
    )
    
    print(f"\n{'é–¾å€¼':<10} {'# äº¤æ˜“æ¬¡æ•¸':<15} {'Win Rate':<15} {'Precision':<15}")
    print("-" * 60)
    for threshold in [0.5, 0.55, 0.6, 0.65, 0.7]:
        num_trades = baseline_confidence["num_trades"][threshold]
        win_rate = baseline_confidence["win_rate"][threshold]
        precision = baseline_confidence["precision"][threshold]
        
        win_rate_str = f"{win_rate:.4f}" if not np.isnan(win_rate) else "N/A"
        precision_str = f"{precision:.4f}" if not np.isnan(precision) else "N/A"
        
        print(f"{threshold:<10.2f} {num_trades:<15} {win_rate_str:<15} {precision_str:<15}")

    return {
        "quantum": quantum_results,
        "baseline": pure_results,
        "quantum_confidence": quantum_confidence,
        "baseline_confidence": baseline_confidence,
    }


def main() -> None:
    """ä¸»å‡½æ•¸ï¼šåŸ·è¡Œå¥å…¨æ€§æª¢æŸ¥å’ŒåŸºæº–æ¯”è¼ƒ."""
    # æ­¥é©Ÿ 1: å¥å…¨æ€§æª¢æŸ¥
    sanity_passed = run_sanity_check()

    if not sanity_passed:
        print("[WARNING] å¥å…¨æ€§æª¢æŸ¥å¤±æ•—ï¼")
        print("å»ºè­°ï¼šæª¢æŸ¥æ¨¡å‹å¯¦ä½œã€è³‡æ–™è™•ç†æµç¨‹æˆ–ç¨‹å¼ç¢¼éŒ¯èª¤ã€‚")
        print("å¯¦é©—å°‡ç¹¼çºŒåŸ·è¡Œï¼Œä½†çµæœå¯èƒ½ä¸å¯é ã€‚\n")
    else:
        print()

    # æ­¥é©Ÿ 2: åŸºæº–æ¯”è¼ƒ
    try:
        results = run_comparison()

        # æ­¥é©Ÿ 3: çµæœæ¯”è¼ƒ
        print("\n" + "=" * 60)
        print("å¯¦é©—çµæœæ¯”è¼ƒ")
        print("=" * 60)

        quantum_val_loss = results["quantum"]["val_loss"]
        quantum_val_acc = results["quantum"]["val_accuracy"]
        quantum_precision = results["quantum"]["precision"]
        quantum_recall = results["quantum"]["recall"]
        quantum_f1 = results["quantum"]["f1_score"]
        
        baseline_val_loss = results["baseline"]["val_loss"]
        baseline_val_acc = results["baseline"]["val_accuracy"]
        baseline_precision = results["baseline"]["precision"]
        baseline_recall = results["baseline"]["recall"]
        baseline_f1 = results["baseline"]["f1_score"]

        print(f"\n{'æŒ‡æ¨™':<20} {'QuantumLSTM':<20} {'PureLSTM (Baseline)':<20}")
        print("-" * 80)
        print(f"{'é©—è­‰æå¤±':<20} {quantum_val_loss:<20.4f} {baseline_val_loss:<20.4f}")
        print(f"{'é©—è­‰æº–ç¢ºç‡':<20} {quantum_val_acc:<20.4f} {baseline_val_acc:<20.4f}")
        print(f"{'Precision (é«˜æ³¢å‹•)':<20} {quantum_precision:<20.4f} {baseline_precision:<20.4f}")
        print(f"{'Recall (é«˜æ³¢å‹•)':<20} {quantum_recall:<20.4f} {baseline_recall:<20.4f}")
        print(f"{'F1-Score':<20} {quantum_f1:<20.4f} {baseline_f1:<20.4f}")

        improvement_loss = baseline_val_loss - quantum_val_loss
        improvement_acc = quantum_val_acc - baseline_val_acc
        improvement_precision = quantum_precision - baseline_precision
        improvement_recall = quantum_recall - baseline_recall

        print(f"\næ”¹å–„å¹…åº¦ï¼š")
        print(f"  é©—è­‰æå¤±æ”¹å–„: {improvement_loss:+.4f} ({improvement_loss/baseline_val_loss*100:+.2f}%)")
        print(f"  é©—è­‰æº–ç¢ºç‡æ”¹å–„: {improvement_acc:+.4f} ({improvement_acc/baseline_val_acc*100:+.2f}%)")
        print(f"  Precision æ”¹å–„: {improvement_precision:+.4f} ({improvement_precision/baseline_precision*100:+.2f}%)")
        print(f"  Recall æ”¹å–„: {improvement_recall:+.4f} ({improvement_recall/baseline_recall*100:+.2f}%)")

        # çµè«–
        print("\n" + "=" * 60)
        print("çµè«– (Conclusion)")
        print("=" * 60)

        if quantum_val_loss < baseline_val_loss:
            print("[SUCCESS] QuantumLSTM å„ªæ–¼ Baseline")
            print(f"   æ˜“ç¶“ç‰¹å¾µï¼ˆæ‰‹å·¥ç‰¹å¾µå·¥ç¨‹ï¼‰å…·æœ‰é æ¸¬æ³¢å‹•æ€§çªç ´çš„èƒ½åŠ›")
            print(f"   é©—è­‰æå¤±é™ä½äº† {abs(improvement_loss):.4f} ({abs(improvement_loss/baseline_val_loss*100):.2f}%)")
            if improvement_recall > 0:
                print(f"   â­ Recall æå‡ {improvement_recall:.4f} - èƒ½æ›´å¥½åœ°æ•æ‰é«˜æ³¢å‹•äº‹ä»¶")
            if improvement_precision > 0:
                print(f"   â­ Precision æå‡ {improvement_precision:.4f} - é æ¸¬çš„é«˜æ³¢å‹•æ›´æº–ç¢º")
        elif quantum_val_loss > baseline_val_loss:
            print("âŒ Baseline å„ªæ–¼ QuantumLSTM")
            print(f"   æ˜“ç¶“ç‰¹å¾µå¯èƒ½æ²’æœ‰é æ¸¬æ³¢å‹•æ€§çªç ´çš„èƒ½åŠ›ï¼Œæˆ–éœ€è¦é€²ä¸€æ­¥å„ªåŒ–")
            print(f"   é©—è­‰æå¤±å¢åŠ äº† {abs(improvement_loss):.4f} ({abs(improvement_loss/baseline_val_loss*100):.2f}%)")
        else:
            print("â– å…©å€‹æ¨¡å‹è¡¨ç¾ç›¸ç•¶")
            print(f"   æ˜“ç¶“ç‰¹å¾µçš„é æ¸¬èƒ½åŠ›æœ‰é™ï¼Œæˆ–éœ€è¦èª¿æ•´æ¨¡å‹æ¶æ§‹")
        
        print(f"\nğŸ’¡ è§£è®€ï¼š")
        print(f"   - Precision: é æ¸¬ç‚ºé«˜æ³¢å‹•æ™‚ï¼Œå¯¦éš›ç‚ºé«˜æ³¢å‹•çš„æ¯”ä¾‹ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰")
        print(f"   - Recall: å¯¦éš›é«˜æ³¢å‹•äº‹ä»¶ä¸­ï¼Œè¢«æ­£ç¢ºé æ¸¬çš„æ¯”ä¾‹ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰")
        print(f"   - å°æ–¼æ³¢å‹•æ€§ç­–ç•¥ï¼ŒRecall æ›´é‡è¦ï¼ˆä¸èƒ½éŒ¯éå¤§æ³¢å‹•ï¼‰")

        # æ­¥é©Ÿ 4: ä¿¡å¿ƒé–¾å€¼æ¯”è¼ƒåˆ†æ
        print("\n" + "=" * 80)
        print("ä¿¡å¿ƒé–¾å€¼æ¯”è¼ƒåˆ†æ (Confidence Tier Comparison)")
        print("=" * 80)
        print("\nç›®æ¨™ï¼šé©—è­‰é«˜ä¿¡å¿ƒé æ¸¬æ˜¯å¦å…·æœ‰æ›´é«˜çš„æº–ç¢ºç‡")
        print("å‡è¨­ï¼šå¦‚æœ QuantumLSTM çš„ Win Rate éš¨é–¾å€¼å¢åŠ è€Œä¸Šå‡ï¼Œ")
        print("      èªªæ˜æ˜“ç¶“ç‰¹å¾µåœ¨é«˜ä¿¡å¿ƒå€é–“å…·æœ‰ Alpha\n")
        
        quantum_confidence = results.get("quantum_confidence", {})
        baseline_confidence = results.get("baseline_confidence", {})
        
        if quantum_confidence and baseline_confidence:
            print(f"{'é–¾å€¼':<10} {'QuantumLSTM':<40} {'PureLSTM (Baseline)':<40}")
            print(f"{'':<10} {'# äº¤æ˜“':<12} {'Win Rate':<12} {'Precision':<12} {'# äº¤æ˜“':<12} {'Win Rate':<12} {'Precision':<12}")
            print("-" * 80)
            
            for threshold in [0.5, 0.55, 0.6, 0.65, 0.7]:
                q_trades = quantum_confidence["num_trades"].get(threshold, 0)
                q_win_rate = quantum_confidence["win_rate"].get(threshold, float('nan'))
                q_precision = quantum_confidence["precision"].get(threshold, float('nan'))
                
                b_trades = baseline_confidence["num_trades"].get(threshold, 0)
                b_win_rate = baseline_confidence["win_rate"].get(threshold, float('nan'))
                b_precision = baseline_confidence["precision"].get(threshold, float('nan'))
                
                q_win_rate_str = f"{q_win_rate:.4f}" if not np.isnan(q_win_rate) else "N/A"
                q_precision_str = f"{q_precision:.4f}" if not np.isnan(q_precision) else "N/A"
                b_win_rate_str = f"{b_win_rate:.4f}" if not np.isnan(b_win_rate) else "N/A"
                b_precision_str = f"{b_precision:.4f}" if not np.isnan(b_precision) else "N/A"
                
                print(f"{threshold:<10.2f} {q_trades:<12} {q_win_rate_str:<12} {q_precision_str:<12} "
                      f"{b_trades:<12} {b_win_rate_str:<12} {b_precision_str:<12}")
            
            # åˆ†æè¶¨å‹¢
            print("\n" + "-" * 80)
            print("è¶¨å‹¢åˆ†æï¼š")
            
            # è¨ˆç®— Win Rate çš„æ–œç‡ï¼ˆå¾ 0.5 åˆ° 0.7ï¼‰
            quantum_win_rates = []
            baseline_win_rates = []
            thresholds_list = [0.5, 0.55, 0.6, 0.65, 0.7]
            
            for threshold in thresholds_list:
                q_wr = quantum_confidence["win_rate"].get(threshold, float('nan'))
                b_wr = baseline_confidence["win_rate"].get(threshold, float('nan'))
                if not np.isnan(q_wr):
                    quantum_win_rates.append(q_wr)
                if not np.isnan(b_wr):
                    baseline_win_rates.append(b_wr)
            
            if len(quantum_win_rates) >= 2 and len(baseline_win_rates) >= 2:
                # è¨ˆç®—ç°¡å–®ç·šæ€§è¶¨å‹¢ï¼ˆæœ€å¾Œå€¼ - ç¬¬ä¸€å€‹å€¼ï¼‰
                quantum_slope = quantum_win_rates[-1] - quantum_win_rates[0] if len(quantum_win_rates) >= 2 else 0
                baseline_slope = baseline_win_rates[-1] - baseline_win_rates[0] if len(baseline_win_rates) >= 2 else 0
                
                print(f"  QuantumLSTM Win Rate è®ŠåŒ–: {quantum_win_rates[0]:.4f} â†’ {quantum_win_rates[-1]:.4f} "
                      f"(æ–œç‡: {quantum_slope:+.4f})")
                print(f"  PureLSTM Win Rate è®ŠåŒ–: {baseline_win_rates[0]:.4f} â†’ {baseline_win_rates[-1]:.4f} "
                      f"(æ–œç‡: {baseline_slope:+.4f})")
                
                if quantum_slope > baseline_slope and quantum_slope > 0:
                    print(f"\n  [SUCCESS] é©—è­‰å‡è¨­ï¼šQuantumLSTM çš„ Win Rate éš¨ä¿¡å¿ƒé–¾å€¼å¢åŠ è€Œä¸Šå‡")
                    print(f"     é€™è­‰æ˜æ˜“ç¶“ç‰¹å¾µåœ¨é«˜ä¿¡å¿ƒå€é–“å…·æœ‰ Alphaï¼ˆè¶…é¡æ”¶ç›Šï¼‰")
                    print(f"     å»ºè­°ï¼šä½¿ç”¨æ›´é«˜çš„ä¿¡å¿ƒé–¾å€¼ï¼ˆå¦‚ 0.65-0.7ï¼‰é€²è¡Œå¯¦éš›äº¤æ˜“")
                elif quantum_slope > 0:
                    print(f"\n  âš ï¸  QuantumLSTM çš„ Win Rate æœ‰ä¸Šå‡è¶¨å‹¢ï¼Œä½†æ”¹å–„å¹…åº¦æœ‰é™")
                else:
                    print(f"\n  âŒ QuantumLSTM çš„ Win Rate æœªéš¨ä¿¡å¿ƒé–¾å€¼å¢åŠ è€Œä¸Šå‡")
                    print(f"     æ˜“ç¶“ç‰¹å¾µå¯èƒ½ä¸å…·å‚™é«˜ä¿¡å¿ƒ Alpha")
        else:
            print("[WARNING] ç„¡æ³•ç²å–ä¿¡å¿ƒé–¾å€¼åˆ†æçµæœ")

        if abs(improvement_loss) < 0.001:
            print("\nâš ï¸  æ³¨æ„ï¼šæ”¹å–„å¹…åº¦å¾ˆå°ï¼ˆ< 0.001ï¼‰ï¼Œå¯èƒ½æ²’æœ‰çµ±è¨ˆé¡¯è‘—æ€§")
            print("   å»ºè­°ï¼š")
            print("   1. å¢åŠ è¨“ç·´è³‡æ–™é‡")
            print("   2. ä½¿ç”¨äº¤å‰é©—è­‰é€²è¡Œæ›´åš´æ ¼çš„è©•ä¼°")
            print("   3. æª¢æŸ¥ç‰¹å¾µå·¥ç¨‹æ˜¯å¦æ­£ç¢º")

    except Exception as e:
        print(f"\n[ERROR] å¯¦é©—åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
